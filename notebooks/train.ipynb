{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786b5bdf-288e-42a8-bc03-087a743e69df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 03:19:43 WARN Utils: Your hostname, DESKTOP-DD5N9SQ resolves to a loopback address: 127.0.1.1; using 172.30.167.134 instead (on interface eth0)\n",
      "25/07/08 03:19:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/08 03:20:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US_Accidents_Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78230bd7-999a-4af1-9b17-104f58bc7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = spark.read.parquet(\"data/df_final.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f878145b-f953-46a1-96c1-308a45976e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer la colonne \"Severity\" en \"label\" AVANT le split\n",
    "df_final = df_final.withColumnRenamed(\"Severity\", \"label\")\n",
    "\n",
    "# (Si tu as d√©j√† fait le split, refais-le apr√®s le renommage)\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d9cf36-b5ec-4672-be01-2c0cf925962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 03:23:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/07/08 03:23:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "[Stage 15:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Accuracy: 0.9169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1. D√©finir le mod√®le\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# 2. Entra√Æner le mod√®le\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# 3. Faire des pr√©dictions\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# 4. √âvaluer avec l'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9be7106-9420-4cba-9e1d-2337962abc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 03:28:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 17:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                            |label|prediction|probability                                                                                              |\n",
      "+------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "|[28.9,20.4,100.0,30.19,0.8,9.2,0.01,0.01,55.0,47.0,5.0,15.0,8.0,1.0]                |1    |2.0       |[2.9667021141083517E-7,6.825624351801085E-4,0.9193214615575306,0.05418348848106185,0.02581219085601594]  |\n",
      "|[52.0,52.0,50.0,29.42,10.0,6.0,0.0,0.855,31.0,28.0,21.0,9.0,0.0,0.0]                |1    |2.0       |[4.0220870120860637E-7,0.0012989568151752836,0.8822447007205633,0.08657395890241543,0.029881981353144814]|\n",
      "|[50.0,50.0,87.0,27.01,2.0,17.0,0.0,0.003,21.0,41.0,13.0,6.0,14.0,1.0]               |1    |2.0       |[2.2242391907913014E-7,6.106383487060807E-4,0.9557713266094062,0.021075176610554212,0.02254263600741451] |\n",
      "|[73.0,73.0,73.0,30.27,10.0,0.0,0.0,0.008,1069.0,18.0,1.0,0.0,5.0,1.0]               |1    |2.0       |[2.3682836796967124E-7,8.464669975800512E-4,0.9549424486029324,0.015188758191488235,0.029022089379631327]|\n",
      "|[49.0,42.0,50.0,29.58,10.0,20.0,0.0,2.104,55.0,286.0,32.0,3.0,0.0,1.0]              |1    |2.0       |[4.978374945282612E-7,0.0013160522861792256,0.832876507125699,0.10578640403057088,0.06002053872005627]   |\n",
      "|[74.0,74.0,73.0,30.16,10.0,9.0,0.0,0.003,27.0,19.0,1.0,14.0,0.0,1.0]                |1    |2.0       |[2.7571001950761386E-7,9.813788743079678E-4,0.9456315134709976,0.029385585737586272,0.02400124620708867] |\n",
      "|[87.0,87.0,44.0,30.1,10.0,6.0,0.0,2.569,14.0,34.0,1.0,13.0,1.0,0.0]                 |1    |2.0       |[3.164286766044396E-7,0.0014290467290968912,0.9386271989578674,0.037164011915959576,0.0227794259683995]  |\n",
      "|[57.0,57.0,100.0,28.98,2.0,0.0,0.0,0.7440000000000001,1325.0,26.0,12.0,0.0,10.0,0.0]|1    |2.0       |[2.7123532111392763E-7,8.829160661457419E-4,0.9447757954217619,0.025378798137962427,0.028962219138808892]|\n",
      "|[45.0,41.0,93.0,29.57,4.0,7.0,0.16,3.746,760.0,97.0,4.0,15.0,11.0,1.0]              |1    |2.0       |[3.8163841012611783E-7,0.0010475529999363971,0.8947928854738485,0.06425682103156288,0.03990235885624209] |\n",
      "|[77.0,77.0,78.0,30.0,10.0,3.0,0.0,2.119,22.0,18.0,1.0,6.0,3.0,0.0]                  |1    |2.0       |[2.946694010026244E-7,0.0012437409176881034,0.9442178208180346,0.03132068652329949,0.023217457071576655] |\n",
      "+------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Afficher un √©chantillon de 10 pr√©dictions avec d√©tails\n",
    "predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4cb5e3b-9cb2-4f85-a4a5-c73169288c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                 |label|prediction|probability                                                                                              |\n",
      "+-------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "|[21.9,9.8,92.0,29.71,2.0,12.7,0.01,0.01,58.0,5.0,19.0,6.0,8.0,1.0]       |2    |2.0       |[3.3597154327465493E-7,7.179524448202113E-4,0.8933826876629855,0.07646553647433249,0.029433487446318414] |\n",
      "|[15.4,1.0,82.0,29.95,7.0,13.8,0.0,0.01,3322.0,98.0,19.0,7.0,8.0,1.0]     |2    |2.0       |[4.52549375814635E-7,8.727129137294235E-4,0.835761532968183,0.09417584802002753,0.06918945354868408]     |\n",
      "|[6.1,-6.8,83.0,30.22,10.0,8.1,0.0,0.01,56.0,64.0,19.0,20.0,3.0,1.0]      |2    |2.0       |[4.823955951826818E-7,9.075861652120175E-4,0.7313556223242172,0.22701699911109274,0.04071931000388289]   |\n",
      "|[12.0,0.5,80.0,30.08,2.0,8.1,0.0,0.451,284.0,142.0,19.0,20.0,8.0,0.0]    |2    |2.0       |[4.515250019124922E-7,9.425698548460134E-4,0.7778280278151958,0.18684877402212546,0.034380176782830923]  |\n",
      "|[21.6,13.0,49.0,30.3,10.0,6.9,0.0,0.01,58.0,5.0,19.0,20.0,1.0,0.0]       |2    |2.0       |[4.624001774990222E-7,0.0011257635455611714,0.7578673114748217,0.21242769405146447,0.0285787685279751]   |\n",
      "|[32.0,24.8,85.0,29.99,5.0,8.1,0.0,0.01,3167.0,584.0,19.0,21.0,6.0,0.0]   |2    |2.0       |[5.644099136448497E-7,0.0012353907118623525,0.7800245568299534,0.10403525484070897,0.11470423320756157]  |\n",
      "|[32.0,24.2,75.0,29.79,9.0,9.2,0.0,0.265,3684.0,401.0,19.0,3.0,6.0,0.0]   |2    |2.0       |[4.7143237845044776E-7,0.0011370920634939195,0.8419125531641131,0.06080679106475722,0.09614309227525743] |\n",
      "|[36.0,33.3,79.0,30.17,10.0,3.5,0.0,0.01,5445.0,159.0,19.0,16.0,4.0,0.0]  |2    |2.0       |[5.871367790173542E-7,0.00144307009292756,0.7795774132478098,0.11380606912300992,0.1051728603994737]     |\n",
      "|[39.9,33.4,86.0,29.67,4.0,10.4,0.18,0.407,284.0,142.0,19.0,23.0,11.0,1.0]|2    |2.0       |[4.5609912830192656E-7,0.0010630285393919963,0.8237746295688462,0.13309111715176838,0.042070768640864974]|\n",
      "|[34.0,23.8,96.0,29.61,2.5,16.1,0.08,1.318,120.0,147.0,19.0,17.0,11.0,0.0]|2    |2.0       |[4.478970312118914E-7,0.001123576782442655,0.8165965497750666,0.14890188833503287,0.03337753721042671]   |\n",
      "+-------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Filtrer o√π prediction == label\n",
    "correct_preds = predictions.filter(predictions.prediction == predictions.label)\n",
    "\n",
    "# Afficher 10 lignes\n",
    "correct_preds.select(\"features\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ceeb0d-4824-4846-93d5-462a7cb5a8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr√©cision manuelle : 649027/707880 = 0.9169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Nombre d'exemples correctement class√©s\n",
    "correct = predictions.filter(predictions.label == predictions.prediction).count()\n",
    "\n",
    "# Total d'exemples test√©s\n",
    "total = predictions.count()\n",
    "\n",
    "print(f\"Pr√©cision manuelle : {correct}/{total} = {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125f79e7-4711-44d5-9f55-b58ace98d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "lr_model.save(\"models/logistic_model_us_accidents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "413293ab-968a-449d-be9d-344e221024b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                            |label|prediction|probability                                                                                              |\n",
      "+------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "|[28.9,20.4,100.0,30.19,0.8,9.2,0.01,0.01,55.0,47.0,5.0,15.0,8.0,1.0]                |1    |2.0       |[2.966702114108357E-7,6.825624351801061E-4,0.9193214615575306,0.05418348848106207,0.025812190856015802]  |\n",
      "|[52.0,52.0,50.0,29.42,10.0,6.0,0.0,0.855,31.0,28.0,21.0,9.0,0.0,0.0]                |1    |2.0       |[4.0220870120860537E-7,0.0012989568151752793,0.8822447007205643,0.08657395890241448,0.029881981353144783]|\n",
      "|[50.0,50.0,87.0,27.01,2.0,17.0,0.0,0.003,21.0,41.0,13.0,6.0,14.0,1.0]               |1    |2.0       |[2.224239190791263E-7,6.106383487060685E-4,0.9557713266094067,0.021075176610553966,0.022542636007414383] |\n",
      "|[73.0,73.0,73.0,30.27,10.0,0.0,0.0,0.008,1069.0,18.0,1.0,0.0,5.0,1.0]               |1    |2.0       |[2.3682836796966938E-7,8.464669975800453E-4,0.9549424486029335,0.01518875819148794,0.02902208937963059]  |\n",
      "|[49.0,42.0,50.0,29.58,10.0,20.0,0.0,2.104,55.0,286.0,32.0,3.0,0.0,1.0]              |1    |2.0       |[4.978374945282607E-7,0.0013160522861792197,0.8328765071256997,0.10578640403057045,0.06002053872005628]  |\n",
      "|[74.0,74.0,73.0,30.16,10.0,9.0,0.0,0.003,27.0,19.0,1.0,14.0,0.0,1.0]                |1    |2.0       |[2.7571001950761206E-7,9.813788743079656E-4,0.9456315134709982,0.02938558573758591,0.024001246207088385] |\n",
      "|[87.0,87.0,44.0,30.1,10.0,6.0,0.0,2.569,14.0,34.0,1.0,13.0,1.0,0.0]                 |1    |2.0       |[3.164286766044382E-7,0.001429046729096895,0.9386271989578682,0.03716401191595903,0.022779425968399252]  |\n",
      "|[57.0,57.0,100.0,28.98,2.0,0.0,0.0,0.7440000000000001,1325.0,26.0,12.0,0.0,10.0,0.0]|1    |2.0       |[2.712353211139241E-7,8.829160661457326E-4,0.9447757954217629,0.025378798137962035,0.028962219138808396] |\n",
      "|[45.0,41.0,93.0,29.57,4.0,7.0,0.16,3.746,760.0,97.0,4.0,15.0,11.0,1.0]              |1    |2.0       |[3.8163841012611725E-7,0.0010475529999363937,0.8947928854738487,0.06425682103156294,0.03990235885624189] |\n",
      "|[77.0,77.0,78.0,30.0,10.0,3.0,0.0,2.119,22.0,18.0,1.0,6.0,3.0,0.0]                  |1    |2.0       |[2.946694010026221E-7,0.0012437409176881014,0.9442178208180356,0.031320686523298946,0.023217457071576287]|\n",
      "+------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "|features                                                              |label|prediction|probability                                                                                              |\n",
      "+----------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "|[28.9,20.4,100.0,30.19,0.8,9.2,0.01,0.01,55.0,47.0,5.0,15.0,8.0,1.0]  |1    |2.0       |[2.966702114108357E-7,6.825624351801061E-4,0.9193214615575306,0.05418348848106207,0.025812190856015802]  |\n",
      "|[52.0,52.0,50.0,29.42,10.0,6.0,0.0,0.855,31.0,28.0,21.0,9.0,0.0,0.0]  |1    |2.0       |[4.0220870120860537E-7,0.0012989568151752793,0.8822447007205643,0.08657395890241448,0.029881981353144783]|\n",
      "|[50.0,50.0,87.0,27.01,2.0,17.0,0.0,0.003,21.0,41.0,13.0,6.0,14.0,1.0] |1    |2.0       |[2.224239190791263E-7,6.106383487060685E-4,0.9557713266094067,0.021075176610553966,0.022542636007414383] |\n",
      "|[73.0,73.0,73.0,30.27,10.0,0.0,0.0,0.008,1069.0,18.0,1.0,0.0,5.0,1.0] |1    |2.0       |[2.3682836796966938E-7,8.464669975800453E-4,0.9549424486029335,0.01518875819148794,0.02902208937963059]  |\n",
      "|[49.0,42.0,50.0,29.58,10.0,20.0,0.0,2.104,55.0,286.0,32.0,3.0,0.0,1.0]|1    |2.0       |[4.978374945282607E-7,0.0013160522861792197,0.8328765071256997,0.10578640403057045,0.06002053872005628]  |\n",
      "+----------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "# Exemple : prendre 5 lignes de test_data\n",
    "some_data = test_data.limit(5)\n",
    "\n",
    "# Faire des pr√©dictions\n",
    "some_predictions = lr_model.transform(some_data)\n",
    "some_predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf28a52-596a-4b48-b8ba-43722adbb692",
   "metadata": {},
   "source": [
    "## üìä Mod√®le de R√©gression Logistique avec PySpark\n",
    "\n",
    "### üîß Pr√©paration des donn√©es\n",
    "- Nettoyage d'un jeu de donn√©es de pr√®s de 4 millions de lignes.\n",
    "- Encodage des variables cat√©gorielles √† l‚Äôaide de `StringIndexer`.\n",
    "- Assemblage des variables num√©riques et index√©es avec `VectorAssembler`.\n",
    "\n",
    "### üß† Entra√Ænement du mod√®le\n",
    "- Utilisation d‚Äôun **mod√®le de r√©gression logistique** via `pyspark.ml.classification.LogisticRegression`.\n",
    "- Le mod√®le a √©t√© entra√Æn√© sur un jeu de donn√©es d'entra√Ænement issu d‚Äôun `randomSplit` (80% entra√Ænement, 20% test).\n",
    "\n",
    "### ‚úÖ R√©sultats\n",
    "- **Accuracy obtenue** sur le jeu de test : **91,69%**\n",
    "- Test sur un petit √©chantillon (5 lignes) :\n",
    "  - Les pr√©dictions sont majoritairement correctes.\n",
    "  - Les probabilit√©s montrent une forte confiance du mod√®le dans ses d√©cisions.\n",
    "\n",
    "### üìå Conclusion\n",
    "- La r√©gression logistique s‚Äôest montr√©e efficace pour ce probl√®me de classification.\n",
    "- Ce r√©sultat confirme l'importance du **pr√©traitement rigoureux** et du **choix des variables** dans la r√©ussite du mod√®le.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a44ba-dc49-4bca-b78b-f9989e2bd1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 4) / 5]"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# ‚öôÔ∏è Initialiser avec maxBins augment√©\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=10, maxBins=13000)\n",
    "\n",
    "# üß† Entra√Æner\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# üîÆ Pr√©dictions\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# ‚úÖ Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = evaluator.evaluate(dt_predictions)\n",
    "\n",
    "print(f\"‚úÖ Decision Tree Accuracy: {dt_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f37ac43f-28db-4079-9453-9be4c05f3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "dt_model.save(\"models/decision_tree_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e496ac2c-df23-4faf-8f1b-38830909c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 01:05:45 WARN MemoryStore: Not enough space to cache rdd_208_3 in memory! (computed 151.6 MiB so far)\n",
      "25/07/07 01:05:45 WARN MemoryStore: Not enough space to cache rdd_208_2 in memory! (computed 151.6 MiB so far)\n",
      "25/07/07 01:05:45 WARN BlockManager: Persisting block rdd_208_2 to disk instead.\n",
      "25/07/07 01:05:45 WARN BlockManager: Persisting block rdd_208_3 to disk instead.\n",
      "25/07/07 01:05:45 WARN MemoryStore: Not enough space to cache rdd_208_0 in memory! (computed 151.6 MiB so far)\n",
      "25/07/07 01:05:45 WARN BlockManager: Persisting block rdd_208_0 to disk instead.\n",
      "25/07/07 01:07:05 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/07/07 01:08:25 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/07/07 01:10:38 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "25/07/07 01:13:28 WARN DAGScheduler: Broadcasting large task binary with size 12.3 MiB\n",
      "25/07/07 01:17:39 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "25/07/07 01:20:17 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "25/07/07 01:22:38 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/07/07 01:24:31 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/07/07 01:26:25 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/07/07 01:28:09 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/07/07 01:30:08 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/07/07 01:32:09 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/07/07 01:33:55 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/07/07 01:35:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/07/07 01:37:28 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/07/07 01:39:17 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n",
      "25/07/07 01:41:29 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/07/07 01:43:33 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/07/07 01:45:26 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/07/07 01:47:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/07/07 01:48:56 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/07/07 01:50:44 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/07/07 01:52:24 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/07/07 01:54:14 WARN DAGScheduler: Broadcasting large task binary with size 6.8 MiB\n",
      "25/07/07 01:56:03 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/07/07 01:57:56 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/07/07 01:59:51 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/07/07 02:01:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/07/07 02:03:44 WARN DAGScheduler: Broadcasting large task binary with size 7.0 MiB\n",
      "25/07/07 02:06:16 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n",
      "25/07/07 02:08:34 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/07/07 02:10:14 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/07/07 02:11:52 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/07/07 02:13:33 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/07/07 02:15:16 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/07/07 02:16:47 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/07/07 14:58:02 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/07/07 15:00:01 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/07/07 15:01:47 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/07/07 15:03:31 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/07/07 15:05:22 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/07/07 15:07:25 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/07/07 15:09:25 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/07/07 15:13:14 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/07/07 15:16:46 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/07/07 15:18:59 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/07/07 15:21:01 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/07/07 15:22:56 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/07/07 15:24:55 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/07/07 15:26:44 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/07/07 15:28:41 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/07/07 15:30:36 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n",
      "25/07/07 15:32:42 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/07/07 15:33:54 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/07/07 15:35:46 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/07/07 15:37:37 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/07/07 15:39:39 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/07/07 15:41:24 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/07/07 15:42:36 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/07/07 15:44:22 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/07/07 15:46:12 WARN DAGScheduler: Broadcasting large task binary with size 49.0 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random Forest Accuracy: 0.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 15:47:41 WARN TaskSetManager: Stage 203 contains a task of very large size (12011 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# ‚öôÔ∏è Cr√©er le mod√®le avec maxBins suffisamment √©lev√©\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50, maxDepth=10, maxBins=13000)\n",
    "\n",
    "# üß† Entra√Æner\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# üîÆ Pr√©dictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# ‚úÖ Accuracy\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "print(f\"‚úÖ Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "\n",
    "# üíæ Sauvegarde\n",
    "rf_model.save(\"models/random_forest_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d9946b-f897-4259-b0aa-ce2ea79174b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.save(\"models/decision_tree_model_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2459f79-78c6-4252-a9bf-c18a8d0e24c4",
   "metadata": {},
   "source": [
    "## ü§ñ Comparaison des Mod√®les de Classification (Logistic Regression, Decision Tree, Random Forest)\n",
    "\n",
    "Dans notre projet d‚Äôanalyse des accidents de la route, nous avons test√© plusieurs mod√®les de classification supervis√©e afin de pr√©dire la **gravit√© des accidents** (`Severity`), une variable num√©rique que nous avons trait√©e comme une **√©tiquette (`label`) cat√©gorielle**.\n",
    "\n",
    "### üéØ Objectif :\n",
    "Pr√©dire le **niveau de gravit√© d‚Äôun accident** (faible √† √©lev√©) en se basant sur les caract√©ristiques comme l‚Äôheure, le lieu, les conditions m√©t√©o, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Mod√®le 1 : **Logistic Regression**\n",
    "\n",
    "- **Type** : Mod√®le lin√©aire probabiliste  \n",
    "- **Principe** : Apprend une fronti√®re de d√©cision lin√©aire entre les classes.  \n",
    "- **Avantage** : Rapide, efficace sur des donn√©es peu complexes.  \n",
    "- **R√©sultat** :  \n",
    "  `‚úÖ Accuracy: 0.9169`\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Mod√®le 2 : **Decision Tree Classifier**\n",
    "\n",
    "- **Type** : Arbre de d√©cision unitaire  \n",
    "- **Principe** : Divise les donn√©es en fonction des attributs en suivant les valeurs qui r√©duisent l'entropie.  \n",
    "- **Avantage** : Facile √† interpr√©ter, fonctionne bien avec des donn√©es non lin√©aires.  \n",
    "- **R√©sultat** :  \n",
    "  `‚úÖ Accuracy: 0.9214`\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Mod√®le 3 : **Random Forest Classifier** *(√† venir)*\n",
    "\n",
    "- **Type** : Ensemble d‚Äôarbres (bagging)  \n",
    "- **Principe** : Combine plusieurs arbres de d√©cision al√©atoires pour am√©liorer la stabilit√© et la pr√©cision.  \n",
    "- **Avantage** : R√©duit le surapprentissage (overfitting), robuste.  \n",
    "- **R√©sultat** : *(sera mesur√©)*\n",
    " `‚úÖ Accuracy: 0.9210`\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Pr√©dictions r√©alis√©es\n",
    "\n",
    "Chaque mod√®le a √©t√© entra√Æn√© sur un jeu de donn√©es d‚Äôaccidents, avec des variables transform√©es (ex. : `features`, `label`), puis √©valu√© sur des donn√©es de test.  \n",
    "Les pr√©dictions permettent de savoir **si un accident est susceptible d‚Äô√™tre grave ou non** en fonction des conditions d√©tect√©es (m√©t√©o, route, etc.).\n",
    "\n",
    "Exemple de pr√©diction (Logistic Regression) :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8a81ac-a948-49a6-8c7d-2f4022b742a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/souhail/bigdata-accidents/notebooks/data/accidents_preprocessed.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReloadModelAndPredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. Charger les donn√©es (remplace par le bon chemin si besoin)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/accidents_preprocessed.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 3. Split en train/test\u001b[39;00m\n\u001b[1;32m     12\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:531\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    520\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    522\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    523\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    529\u001b[0m )\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/souhail/bigdata-accidents/notebooks/data/accidents_preprocessed.parquet."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c8072c-c5e7-45f2-a484-690e3b85ac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/souhail/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/souhail/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/souhail/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1. Initialiser le mod√®le Random Forest\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m rf \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumTrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxDepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxBins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. Entra√Æner le mod√®le\u001b[39;00m\n\u001b[1;32m      9\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mfit(train_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/classification.py:2104\u001b[0m, in \u001b[0;36mRandomForestClassifier.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, numTrees, featureSubsetStrategy, seed, subsamplingRate, leafCol, minWeightFractionPerNode, weightCol, bootstrap)\u001b[0m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;124;03m__init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;124;03m         probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;124;03m         leafCol=\"\", minWeightFractionPerNode=0.0, weightCol=None, bootstrap=True)\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28msuper\u001b[39m(RandomForestClassifier, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m-> 2104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.ml.classification.RandomForestClassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2107\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:84\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjava_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_obj(\u001b[38;5;241m*\u001b[39mjava_args)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: org does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Initialiser le mod√®le Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxDepth=15, maxBins=13000)\n",
    "\n",
    "# 2. Entra√Æner le mod√®le\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# 3. Pr√©dictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# 4. Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "print(f\"üéØ Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "\n",
    "# 5. Affichage des pr√©dictions\n",
    "rf_predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2f366-1ac0-40d5-b7b0-96ad6ecf207c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
